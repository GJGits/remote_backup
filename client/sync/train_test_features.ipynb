{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- IMPORTs -----\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import os\n",
    "import pandas\n",
    "import sklearn.metrics as metric\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler # Necessario per SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- CONSTANTS -----\n",
    "#models = [\"tree\", \"forest\", \"knn\", \"svm-rbf\", \"svm-linear\", \"svm-poly\", \"svm-sigmoid\", \"gb\"]\n",
    "models = [ \"svm-rbf\" ]\n",
    "onlyMinMax = False\n",
    "#datasetDirectory = \"../../04. Features/10. EcgStrict/Analysis/WSMeanMFrequency&Time/\"\n",
    "datasetDirectory = \"datasets/features_post_avgM/\"\n",
    "out_dir = \"datasets/train_test_metrics/{}/\"\n",
    "out_dir2 = \"datasets/train_test_metrics/model/\"\n",
    "#datasetDirectory = \"../../04. Features/07. ConcatenatiCutMonotoni/\"\n",
    "allSubjects = [ \"%02d\"%subjectInt for subjectInt in range(1, 41) ]\n",
    "subjectsToRemove = [\"01\", \"02\", \"40\"]\n",
    "#subjectsToRemove = []\n",
    "subjects = [ subject for subject in allSubjects if subject not in subjectsToRemove ]\n",
    "columns = [\"Timestamp\", \"Label\", \"MaxRR\", \"MeanRR\", \"MeanHR\", \"minRR\", \"minHR\", \"maxHR\", \"RMSSD\", \"RangeRR\", \"NN50\", \"PNN50\", \"StdRR\", \"StdHR\", \"LowFrequenciesPower\", \"HighFrequenciesPower\", \"VeryHighFrequenciesPower\", \"TotalSignalPower\", \"%lowFrequencyPower\", \"%highFrequencyPower\", \"lowFrequencyPower/highFrequencyPower\"]\n",
    "#columns = [\"Timestamp\", \"Label\", \"SCL_mean\", \"EDA_Max-min\", \"SCL_derivative_mean\"]\n",
    "featuresNamesTaken = [\"maxHR\", \"MaxRR\", \"StdHR\", \"lowFrequencyPower/highFrequencyPower\", \"%lowFrequencyPower\", \"VeryHighFrequenciesPower\", \"PNN50\", \"HighFrequenciesPower\", \"StdRR\" ]\n",
    "#featuresNamesTaken = [ \"SCL_mean\", \"EDA_Max-min\", \"SCL_derivative_mean\" ]\n",
    "columnsToDrop = [ column for column in columns if column not in featuresNamesTaken ]\n",
    "labels = [0, 1]\n",
    "metricsNames = [\"Recall-0\", \"Recall-1\", \"Precision-0\", \"Precision-1\", \"Fscore-0\", \"Fscore-1\", \"Accuracy\", \"TN\", \"FP\", \"FN\", \"TP\", \"%TN\", \"%FP\", \"%FN\", \"%TP\", \"CPU-Time\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FUNCTIONS -----\n",
    "def trainAndTest(modelName, testSubject, classifier, metricsDF, scoresDF=None):\n",
    "    modelFilename = \"%s%s_Classifier_tp%s.pkl\" % (out_dir.format(testSubject),modelName, testSubject)\n",
    "    resultFilename = \"%s%s_Results_tp%s.txt\" % (out_dir.format(testSubject),modelName, testSubject)\n",
    "    scalerFilename = \"%s%s_Scaler_tp%s.pkl\" % (out_dir.format(testSubject),modelName, testSubject)\n",
    "    predictionFilename = \"%s%s_PredictedLabels.csv\" % (out_dir.format(testSubject),modelName)\n",
    "    start = time.time()\n",
    "    # Build the trainingSet\n",
    "    trainingSet = None\n",
    "    for subject in subjects:\n",
    "        # Skip test subject\n",
    "        if subject==testSubject:\n",
    "            continue\n",
    "        # Concat the subject data to the training set\n",
    "        filename = datasetDirectory+\"f\"+subject+\".csv\"\n",
    "        subjectFeatures = pandas.read_csv(filename, sep = '\\t')\n",
    "        trainingSet = pandas.concat([trainingSet, subjectFeatures])\n",
    "    # Estraggo l'etichetta Label\n",
    "    trainLabels = trainingSet[\"Label\"]\n",
    "    # Rimuovo le colonne inutili per il training\n",
    "    trainingSet = trainingSet.drop(columns=columnsToDrop)\n",
    "    # Costruisco il modello\n",
    "    if not onlyMinMax and (\"svm\" in modelName or \"knn\" in modelName):\n",
    "        scaler = StandardScaler()\n",
    "        trainingSet = scaler.fit_transform( trainingSet.to_numpy() )\n",
    "        joblib.dump(scaler, scalerFilename)\n",
    "    classifier.fit(trainingSet, trainLabels)\n",
    "    # Salvo il modello\n",
    "    joblib.dump(classifier, modelFilename)\n",
    "    # Load test data\n",
    "    testSet = None\n",
    "    filename = datasetDirectory +\"f\"+testSubject+\".csv\"\n",
    "    testSet = pandas.read_csv(filename, sep = '\\t')\n",
    "    # Estrai la label\n",
    "    trueLabels = testSet[\"Label\"]\n",
    "    # Rimuovo le colonne inutili per il test\n",
    "    testSet = testSet.drop(columns=columnsToDrop)\n",
    "    # Prediction\n",
    "    if not onlyMinMax and (\"svm\" in modelName or \"knn\" in modelName):\n",
    "        testSet = scaler.transform( testSet.to_numpy() )\n",
    "    predictedLabels = classifier.predict(testSet)\n",
    "    # Write predicted labels on a file\n",
    "    predictionFile = open(predictionFilename, \"a+\")\n",
    "    predictionFile.write( testSubject )\n",
    "    for label in predictedLabels:\n",
    "        predictionFile.write( \"\\t\"+str(int(label)) )\n",
    "    predictionFile.write( \"\\n\" )\n",
    "    predictionFile.close()\n",
    "    # Evaluate results\n",
    "    recall = metric.recall_score(trueLabels, predictedLabels, labels=labels, average=None)\n",
    "    precision = metric.precision_score(trueLabels, predictedLabels, labels=labels, average=None)\n",
    "    fmeasure =  metric.f1_score(trueLabels, predictedLabels, labels=labels, average=None)\n",
    "    accuracy = metric.accuracy_score(trueLabels, predictedLabels)\n",
    "    confusionMatrix = metric.confusion_matrix(trueLabels, predictedLabels, labels=labels)\n",
    "    end = time.time()\n",
    "    # Print features importance\n",
    "    if \"forest\" in modelName or \"tree\" in modelName:\n",
    "        scoresDF[\"Subject \"+testSubject] = [ score for score in classifier.feature_importances_ ]\n",
    "    tn, fp, fn, tp = confusionMatrix.ravel()\n",
    "    totWindows100 = 100/(tn+fp+fn+tp)\n",
    "    metricsDF[\"Subject \"+testSubject] = [ recall[0], recall[1], precision[0], precision[1], fmeasure[0], fmeasure[1], accuracy, tn, fp, fn, tp, tn*totWindows100, fp*totWindows100, fn*totWindows100, tp*totWindows100, end-start ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: svm-rbf\n",
      "    Test subject: 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjcode/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/gjcode/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test subject: 04\n",
      "    Test subject: 05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjcode/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/gjcode/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test subject: 06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjcode/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/gjcode/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test subject: 07\n",
      "    Test subject: 08\n",
      "    Test subject: 09\n",
      "    Test subject: 10\n",
      "    Test subject: 11\n",
      "    Test subject: 12\n",
      "    Test subject: 13\n",
      "    Test subject: 14\n",
      "    Test subject: 15\n",
      "    Test subject: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjcode/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/gjcode/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test subject: 17\n",
      "    Test subject: 18\n",
      "    Test subject: 19\n",
      "    Test subject: 20\n",
      "    Test subject: 21\n",
      "    Test subject: 22\n",
      "    Test subject: 23\n",
      "    Test subject: 24\n",
      "    Test subject: 25\n",
      "    Test subject: 26\n",
      "    Test subject: 27\n",
      "    Test subject: 28\n",
      "    Test subject: 29\n",
      "    Test subject: 30\n",
      "    Test subject: 31\n",
      "    Test subject: 32\n",
      "    Test subject: 33\n",
      "    Test subject: 34\n",
      "    Test subject: 35\n",
      "    Test subject: 36\n",
      "    Test subject: 37\n",
      "    Test subject: 38\n",
      "    Test subject: 39\n"
     ]
    }
   ],
   "source": [
    "# ----- MAIN -----\n",
    "# Choose the model to build\n",
    "for model in models:\n",
    "    print( \"Model: \"+model )\n",
    "    metricsDF = pandas.DataFrame()\n",
    "    metricsDF[\"\"] = metricsNames\n",
    "    scoresDF = pandas.DataFrame()\n",
    "    scoresDF[\"\"] = featuresNamesTaken\n",
    "    # Choose the subject for the test\n",
    "    for testSubject in subjects:\n",
    "        print( \"    Test subject: \"+testSubject )\n",
    "        if not os.path.isdir(out_dir.format(testSubject)):\n",
    "            os.makedirs(out_dir.format(testSubject))\n",
    "        if model==\"tree\":\n",
    "            classifier = DecisionTreeClassifier()\n",
    "            trainAndTest(model, testSubject, classifier, metricsDF, scoresDF)\n",
    "        elif model==\"forest\":\n",
    "            classifier = RandomForestClassifier()\n",
    "            trainAndTest(model, testSubject, classifier, metricsDF, scoresDF)\n",
    "        elif \"svm\" in model:\n",
    "            _, kernel = model.split(\"-\")\n",
    "            classifier = SVC(C=1, gamma=0.1, kernel=kernel)\n",
    "            trainAndTest(model, testSubject, classifier, metricsDF)\n",
    "        elif model==\"knn\":\n",
    "            classifier = KNeighborsClassifier()\n",
    "            trainAndTest(model, testSubject, classifier, metricsDF)\n",
    "        elif model==\"gb\":\n",
    "            classifier = GradientBoostingClassifier()\n",
    "            trainAndTest(model, testSubject, classifier, metricsDF)\n",
    "    # Compute the mean value of the metrics\n",
    "    meanValues = []\n",
    "    for i in range(len(metricsNames)):\n",
    "        meanValues.append( numpy.mean(metricsDF.iloc[i, 1:]) )\n",
    "    # Write out the metrics DF\n",
    "    metricsDF.insert(1, \"Mean\", meanValues) # indici=>0\n",
    "    metricsDF.to_csv(out_dir2 + model+\"_metrics.csv\", sep='\\t', index=False)\n",
    "    # Compute the mean value of the scores\n",
    "    meanValues = []\n",
    "    for i in range(len(featuresNamesTaken)):\n",
    "        meanValues.append( numpy.mean(scoresDF.iloc[i, 1:]) )\n",
    "    # Write out the score DF\n",
    "    scoresDF.insert(1, \"Mean\", meanValues)\n",
    "    scoresDF.to_csv(out_dir2 + model+\"_scores.csv\", sep='\\t', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
